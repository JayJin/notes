import os
import time
import logging
import base64
import requests
from prometheus_client import start_http_server, Gauge, Counter

# ë¡œê¹… ì„¤ì • (ë” ìƒì„¸í•˜ê²Œ)
log = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.DEBUG,  # âœ… DEBUG ë ˆë²¨ë¡œ ë³€ê²½
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
DATAHUB_GMS_HOST = os.getenv('DATAHUB_GMS_HOST', 'datahub-gms')
DATAHUB_GMS_PORT = os.getenv('DATAHUB_GMS_PORT', '8080')
GMS_URL = f"http://{DATAHUB_GMS_HOST}:{DATAHUB_GMS_PORT}"

PAGE_SIZE = 500
MAX_RETRIES = 3
RETRY_DELAY = 5
SCRAPE_INTERVAL = int(os.getenv('SCRAPE_INTERVAL', '60'))

# âœ… ì—ëŸ¬ ì¹´ìš´í„° ì¶”ê°€
scrape_errors = Counter('datahub_scrape_errors_total', 'Total number of scrape errors')

# Prometheus Metrics ì •ì˜
total_datasets = Gauge('datahub_dataset_total', 'ì „ì²´ ë°ì´í„°ì…‹ ìˆ˜')
desc_filled_total = Gauge('datahub_dataset_with_description', 'ì„¤ëª…ì´ ìˆëŠ” ë°ì´í„°ì…‹ ìˆ˜')
owner_filled_total = Gauge('datahub_dataset_with_owner', 'ì†Œìœ ìê°€ ìˆëŠ” ë°ì´í„°ì…‹ ìˆ˜')
tag_filled_total = Gauge('datahub_dataset_with_tags', 'íƒœê·¸ê°€ ìˆëŠ” ë°ì´í„°ì…‹ ìˆ˜')
datasets_without_owner = Gauge('datahub_dataset_without_owner', 'ì†Œìœ ìê°€ ì—†ëŠ” ë°ì´í„°ì…‹ ìˆ˜')
datasets_without_description = Gauge('datahub_dataset_without_description', 'ì„¤ëª…ì´ ì—†ëŠ” ë°ì´í„°ì…‹ ìˆ˜')
datasets_without_tags = Gauge('datahub_dataset_without_tags', 'íƒœê·¸ê°€ ì—†ëŠ” ë°ì´í„°ì…‹ ìˆ˜')
metadata_completeness_ratio = Gauge('datahub_metadata_completeness_ratio', 'ë©”íƒ€ë°ì´í„° ì™„ì„±ë„ ë¹„ìœ¨')
last_scrape_success = Gauge('datahub_last_scrape_success', 'ë§ˆì§€ë§‰ ìˆ˜ì§‘ ì„±ê³µ ì—¬ë¶€ (1=ì„±ê³µ, 0=ì‹¤íŒ¨)')


def _auth_header():
    """ì¸ì¦ í—¤ë” ìƒì„± (Basic Auth)"""
    client_id = os.getenv('DATAHUB_CLIENT_ID')
    client_secret = os.getenv('DATAHUB_CLIENT_SECRET')
    
    headers = {"Content-Type": "application/json"}
    
    if client_id and client_secret:
        token = f"{client_id}:{client_secret}"
        b64_token = base64.b64encode(token.encode()).decode()
        headers["Authorization"] = f"Basic {b64_token}"
        log.debug("Using authentication")
    else:
        log.debug("No authentication configured (this is OK for default DataHub setup)")
    
    return headers


def _post(query: str, variables: dict = None):
    """GraphQL ì¿¼ë¦¬ ì‹¤í–‰"""
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            payload = {"query": query}
            if variables:
                payload["variables"] = variables
            
            log.debug(f"Sending GraphQL request to {GMS_URL}/api/graphql")
            log.debug(f"Query: {query[:200]}...")  # ì¿¼ë¦¬ ì¼ë¶€ë§Œ ë¡œê¹…
            
            resp = requests.post(
                f"{GMS_URL}/api/graphql",
                json=payload,
                headers=_auth_header(),
                timeout=30
            )
            
            log.debug(f"Response status code: {resp.status_code}")
            
            resp.raise_for_status()
            
            result = resp.json()
            
            # GraphQL ì—ëŸ¬ ì²´í¬
            if "errors" in result:
                log.error(f"GraphQL errors: {result['errors']}")
                scrape_errors.inc()
                return None
            
            log.debug(f"Successfully received response")
            return result.get("data")
        
        except requests.exceptions.ConnectionError as e:
            log.error(f"Connection error (attempt {attempt}/{MAX_RETRIES}): {e}")
            log.error(f"Cannot connect to {GMS_URL} - check if datahub-gms is running and network is correct")
            scrape_errors.inc()
            if attempt < MAX_RETRIES:
                time.sleep(RETRY_DELAY)
        
        except requests.exceptions.Timeout as e:
            log.warning(f"Request timeout (attempt {attempt}/{MAX_RETRIES}): {e}")
            scrape_errors.inc()
            if attempt < MAX_RETRIES:
                time.sleep(RETRY_DELAY)
        
        except requests.exceptions.HTTPError as e:
            log.error(f"HTTP error (attempt {attempt}/{MAX_RETRIES}): {e}")
            log.error(f"Response content: {resp.text[:500]}")
            scrape_errors.inc()
            if attempt < MAX_RETRIES:
                time.sleep(RETRY_DELAY)
        
        except Exception as e:
            log.error(f"Unexpected error (attempt {attempt}/{MAX_RETRIES}): {e}", exc_info=True)
            scrape_errors.inc()
            return None
    
    log.error(f"Failed after {MAX_RETRIES} retries")
    return None


def test_connection():
    """DataHub GMS ì—°ê²° í…ŒìŠ¤íŠ¸"""
    log.info("Testing connection to DataHub GMS...")
    
    try:
        # ê°„ë‹¨í•œ health check
        resp = requests.get(f"{GMS_URL}/health", timeout=10)
        log.info(f"Health check response: {resp.status_code}")
        
        if resp.status_code == 200:
            log.info("âœ… DataHub GMS is reachable")
            return True
        else:
            log.warning(f"âš ï¸ DataHub GMS returned status {resp.status_code}")
            return False
    
    except requests.exceptions.ConnectionError as e:
        log.error(f"âŒ Cannot connect to DataHub GMS at {GMS_URL}")
        log.error(f"Error: {e}")
        log.error("Check: 1) Is datahub-gms container running? 2) Are containers on same network?")
        return False
    
    except Exception as e:
        log.error(f"âŒ Unexpected error during connection test: {e}")
        return False


def get_dataset_count():
    """ì „ì²´ ë°ì´í„°ì…‹ ìˆ˜ë§Œ ê°€ì ¸ì˜¤ê¸°"""
    query = """
    query {
      search(
        input: {
          type: DATASET
          query: "*"
          start: 0
          count: 1
        }
      ) {
        total
      }
    }
    """
    
    log.info("Fetching total dataset count...")
    payload = _post(query)
    
    if payload and 'search' in payload:
        count = payload['search']['total']
        log.info(f"âœ… Total datasets: {count}")
        return count
    else:
        log.error("âŒ Failed to fetch dataset count")
        return 0


def get_datasets_without_owners():
    """ì†Œìœ ìê°€ ì—†ëŠ” ë°ì´í„°ì…‹ ìˆ˜"""
    query = """
    query {
      search(
        input: {
          type: DATASET
          query: "*"
          filters: [
            {
              field: "hasOwners"
              values: ["false"]
            }
          ]
          start: 0
          count: 1
        }
      ) {
        total
      }
    }
    """
    
    log.info("Fetching datasets without owners...")
    payload = _post(query)
    
    if payload and 'search' in payload:
        count = payload['search']['total']
        log.info(f"âœ… Datasets without owners: {count}")
        return count
    else:
        log.error("âŒ Failed to fetch datasets without owners")
        return 0


def get_datasets_without_description():
    """ì„¤ëª…ì´ ì—†ëŠ” ë°ì´í„°ì…‹ ìˆ˜"""
    query = """
    query {
      search(
        input: {
          type: DATASET
          query: "*"
          filters: [
            {
              field: "hasDescription"
              values: ["false"]
            }
          ]
          start: 0
          count: 1
        }
      ) {
        total
      }
    }
    """
    
    log.info("Fetching datasets without description...")
    payload = _post(query)
    
    if payload and 'search' in payload:
        count = payload['search']['total']
        log.info(f"âœ… Datasets without description: {count}")
        return count
    else:
        log.error("âŒ Failed to fetch datasets without description")
        return 0


def get_datasets_without_tags():
    """íƒœê·¸ê°€ ì—†ëŠ” ë°ì´í„°ì…‹ ìˆ˜"""
    query = """
    query {
      search(
        input: {
          type: DATASET
          query: "*"
          filters: [
            {
              field: "hasTags"
              values: ["false"]
            }
          ]
          start: 0
          count: 1
        }
      ) {
        total
      }
    }
    """
    
    log.info("Fetching datasets without tags...")
    payload = _post(query)
    
    if payload and 'search' in payload:
        count = payload['search']['total']
        log.info(f"âœ… Datasets without tags: {count}")
        return count
    else:
        log.error("âŒ Failed to fetch datasets without tags")
        return 0


def collect_metrics():
    """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ì—…ë°ì´íŠ¸"""
    log.info("=" * 60)
    log.info("Starting metric collection...")
    log.info("=" * 60)
    
    try:
        # 1. ì „ì²´ ë°ì´í„°ì…‹ ìˆ˜
        total = get_dataset_count()
        total_datasets.set(total)
        
        # 2. ì†Œìœ ì ì—†ëŠ” ë°ì´í„°ì…‹
        without_owner = get_datasets_without_owners()
        datasets_without_owner.set(without_owner)
        owner_filled_total.set(max(0, total - without_owner))
        
        # 3. ì„¤ëª… ì—†ëŠ” ë°ì´í„°ì…‹
        without_desc = get_datasets_without_description()
        datasets_without_description.set(without_desc)
        desc_filled_total.set(max(0, total - without_desc))
        
        # 4. íƒœê·¸ ì—†ëŠ” ë°ì´í„°ì…‹
        without_tags = get_datasets_without_tags()
        datasets_without_tags.set(without_tags)
        tag_filled_total.set(max(0, total - without_tags))
        
        # 5. ë©”íƒ€ë°ì´í„° ì™„ì„±ë„ ê³„ì‚°
        if total > 0:
            max_incomplete = max(without_owner, without_desc, without_tags)
            completeness = (total - max_incomplete) / total
            metadata_completeness_ratio.set(completeness)
            log.info(f"ğŸ“Š Metadata completeness ratio: {completeness:.2%}")
        
        last_scrape_success.set(1)
        log.info("=" * 60)
        log.info("âœ… Metric collection completed successfully")
        log.info("=" * 60)
        
    except Exception as e:
        log.error(f"âŒ Error during metric collection: {e}", exc_info=True)
        last_scrape_success.set(0)
        scrape_errors.inc()


if __name__ == "__main__":
    log.info("=" * 60)
    log.info("DataHub Prometheus Exporter Starting...")
    log.info("=" * 60)
    log.info(f"GMS URL: {GMS_URL}")
    log.info(f"Scrape interval: {SCRAPE_INTERVAL} seconds")
    log.info(f"Metrics port: 9105")
    log.info("=" * 60)
    
    # ì—°ê²° í…ŒìŠ¤íŠ¸
    if not test_connection():
        log.error("Cannot connect to DataHub GMS. Exiting...")
        exit(1)
    
    # Prometheus HTTP ì„œë²„ ì‹œì‘
    start_http_server(9105)
    log.info("âœ… Prometheus metrics server started on port 9105")
    log.info(f"   Metrics available at http://localhost:9105/metrics")
    
    # ì´ˆê¸° ë©”íŠ¸ë¦­ ê°’ ì„¤ì •
    total_datasets.set(0)
    desc_filled_total.set(0)
    owner_filled_total.set(0)
    tag_filled_total.set(0)
    datasets_without_owner.set(0)
    datasets_without_description.set(0)
    datasets_without_tags.set(0)
    metadata_completeness_ratio.set(0)
    last_scrape_success.set(0)
    
    log.info("âœ… Initial metrics set")
    
    # ì¦‰ì‹œ ì²« ìˆ˜ì§‘ ì‹¤í–‰
    log.info("Running initial metric collection...")
    try:
        collect_metrics()
    except Exception as e:
        log.error(f"Error in initial collection: {e}", exc_info=True)
    
    # ì£¼ê¸°ì ìœ¼ë¡œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    while True:
        try:
            log.info(f"Sleeping for {SCRAPE_INTERVAL} seconds...")
            time.sleep(SCRAPE_INTERVAL)
            collect_metrics()
        except KeyboardInterrupt:
            log.info("Received interrupt signal. Shutting down...")
            break
        except Exception as e:
            log.error(f"Error in main loop: {e}", exc_info=True)
    log.info("DataHub Prometheus Exporter stopped.")