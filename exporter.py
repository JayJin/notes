import os
import time
import logging
import base64
import requests
from prometheus_client import start_http_server, Gauge, Counter, Info

# ë¡œê¹… ì„¤ì •
log = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
DATAHUB_GMS_HOST = os.getenv('DATAHUB_GMS_HOST', 'datahub-gms')
DATAHUB_GMS_PORT = os.getenv('DATAHUB_GMS_PORT', '8080')
GMS_URL = f"http://{DATAHUB_GMS_HOST}:{DATAHUB_GMS_PORT}"

PAGE_SIZE = 500
MAX_RETRIES = 3
RETRY_DELAY = 5
SCRAPE_INTERVAL = int(os.getenv('SCRAPE_INTERVAL', '60'))

# ========================================
# Prometheus Metrics ì •ì˜
# ========================================

# ì—ëŸ¬ ì¹´ìš´í„°
scrape_errors = Counter('datahub_scrape_errors_total', 'Total scrape errors')

# === ë°ì´í„°ì…‹ ê´€ë ¨ ë©”íŠ¸ë¦­ ===
total_datasets = Gauge('datahub_datasets_total', 'ì „ì²´ ë°ì´í„°ì…‹ ìˆ˜')
datasets_by_platform = Gauge('datahub_datasets_by_platform', 'í”Œë«í¼ë³„ ë°ì´í„°ì…‹ ìˆ˜', ['platform'])
datasets_by_domain = Gauge('datahub_datasets_by_domain', 'ë„ë©”ì¸ë³„ ë°ì´í„°ì…‹ ìˆ˜', ['domain'])
datasets_by_env = Gauge('datahub_datasets_by_env', 'í™˜ê²½ë³„ ë°ì´í„°ì…‹ ìˆ˜ (PROD/DEV/QA)', ['environment'])

# === ë©”íƒ€ë°ì´í„° í’ˆì§ˆ ì§€í‘œ ===
datasets_with_owner = Gauge('datahub_datasets_with_owner', 'ì†Œìœ ìê°€ ìˆëŠ” ë°ì´í„°ì…‹')
datasets_without_owner = Gauge('datahub_datasets_without_owner', 'ì†Œìœ ìê°€ ì—†ëŠ” ë°ì´í„°ì…‹')
datasets_with_description = Gauge('datahub_datasets_with_description', 'ì„¤ëª…ì´ ìˆëŠ” ë°ì´í„°ì…‹')
datasets_without_description = Gauge('datahub_datasets_without_description', 'ì„¤ëª…ì´ ì—†ëŠ” ë°ì´í„°ì…‹')
datasets_with_tags = Gauge('datahub_datasets_with_tags', 'íƒœê·¸ê°€ ìˆëŠ” ë°ì´í„°ì…‹')
datasets_without_tags = Gauge('datahub_datasets_without_tags', 'íƒœê·¸ê°€ ì—†ëŠ” ë°ì´í„°ì…‹')
datasets_with_glossary_terms = Gauge('datahub_datasets_with_glossary_terms', 'ìš©ì–´ì§‘ì´ ìˆëŠ” ë°ì´í„°ì…‹')
datasets_without_glossary_terms = Gauge('datahub_datasets_without_glossary_terms', 'ìš©ì–´ì§‘ì´ ì—†ëŠ” ë°ì´í„°ì…‹')
datasets_with_schema = Gauge('datahub_datasets_with_schema', 'ìŠ¤í‚¤ë§ˆê°€ ìˆëŠ” ë°ì´í„°ì…‹')
datasets_without_schema = Gauge('datahub_datasets_without_schema', 'ìŠ¤í‚¤ë§ˆê°€ ì—†ëŠ” ë°ì´í„°ì…‹')

# === ì»¬ëŸ¼ ë ˆë²¨ ë©”íŠ¸ë¦­ ===
total_columns = Gauge('datahub_columns_total', 'ì „ì²´ ì»¬ëŸ¼ ìˆ˜')
columns_with_description = Gauge('datahub_columns_with_description', 'ì„¤ëª…ì´ ìˆëŠ” ì»¬ëŸ¼ ìˆ˜')
columns_without_description = Gauge('datahub_columns_without_description', 'ì„¤ëª…ì´ ì—†ëŠ” ì»¬ëŸ¼ ìˆ˜')

# === íƒœê·¸ ë° ìš©ì–´ì§‘ ë©”íŠ¸ë¦­ ===
total_tags = Gauge('datahub_tags_total', 'ì „ì²´ íƒœê·¸ ìˆ˜')
total_glossary_terms = Gauge('datahub_glossary_terms_total', 'ì „ì²´ ìš©ì–´ì§‘ ìˆ˜')
total_domains = Gauge('datahub_domains_total', 'ì „ì²´ ë„ë©”ì¸ ìˆ˜')
datasets_by_tag = Gauge('datahub_datasets_by_tag', 'íƒœê·¸ë³„ ë°ì´í„°ì…‹ ìˆ˜', ['tag'])

# === ìµœê·¼ í™œë™ ë©”íŠ¸ë¦­ ===
datasets_updated_last_day = Gauge('datahub_datasets_updated_last_day', 'ìµœê·¼ 1ì¼ ë‚´ ì—…ë°ì´íŠ¸ëœ ë°ì´í„°ì…‹')
datasets_updated_last_week = Gauge('datahub_datasets_updated_last_week', 'ìµœê·¼ 7ì¼ ë‚´ ì—…ë°ì´íŠ¸ëœ ë°ì´í„°ì…‹')
datasets_updated_last_month = Gauge('datahub_datasets_updated_last_month', 'ìµœê·¼ 30ì¼ ë‚´ ì—…ë°ì´íŠ¸ëœ ë°ì´í„°ì…‹')

# === ë°ì´í„° ê³„ë³´(Lineage) ë©”íŠ¸ë¦­ ===
datasets_with_upstream = Gauge('datahub_datasets_with_upstream', 'ìƒìœ„ ì˜ì¡´ì„±ì´ ìˆëŠ” ë°ì´í„°ì…‹')
datasets_with_downstream = Gauge('datahub_datasets_with_downstream', 'í•˜ìœ„ ì˜ì¡´ì„±ì´ ìˆëŠ” ë°ì´í„°ì…‹')

# === í’ˆì§ˆ ì™„ì„±ë„ ì§€í‘œ ===
metadata_completeness_ratio = Gauge('datahub_metadata_completeness_ratio', 'ë©”íƒ€ë°ì´í„° ì™„ì„±ë„ ë¹„ìœ¨ (0~1)')
documentation_coverage_ratio = Gauge('datahub_documentation_coverage_ratio', 'ë¬¸ì„œí™” ì»¤ë²„ë¦¬ì§€ (ì„¤ëª…+ìš©ì–´ì§‘)')
governance_coverage_ratio = Gauge('datahub_governance_coverage_ratio', 'ê±°ë²„ë„ŒìŠ¤ ì»¤ë²„ë¦¬ì§€ (ì†Œìœ ì+íƒœê·¸+ë„ë©”ì¸)')

# === ì‚¬ìš©ì ë° ê·¸ë£¹ ë©”íŠ¸ë¦­ ===
total_users = Gauge('datahub_users_total', 'ì „ì²´ ì‚¬ìš©ì ìˆ˜')
total_groups = Gauge('datahub_groups_total', 'ì „ì²´ ê·¸ë£¹ ìˆ˜')

# === ê¸°íƒ€ ì—”í‹°í‹° ë©”íŠ¸ë¦­ ===
total_dashboards = Gauge('datahub_dashboards_total', 'ì „ì²´ ëŒ€ì‹œë³´ë“œ ìˆ˜')
total_charts = Gauge('datahub_charts_total', 'ì „ì²´ ì°¨íŠ¸ ìˆ˜')
total_data_jobs = Gauge('datahub_data_jobs_total', 'ì „ì²´ ë°ì´í„° ì‘ì—… ìˆ˜')
total_data_flows = Gauge('datahub_data_flows_total', 'ì „ì²´ ë°ì´í„° í”Œë¡œìš° ìˆ˜')
total_ml_models = Gauge('datahub_ml_models_total', 'ì „ì²´ ML ëª¨ë¸ ìˆ˜')
total_ml_features = Gauge('datahub_ml_features_total', 'ì „ì²´ ML í”¼ì²˜ ìˆ˜')

# === ì‹œìŠ¤í…œ ìƒíƒœ ===
last_scrape_success = Gauge('datahub_last_scrape_success', 'ë§ˆì§€ë§‰ ìˆ˜ì§‘ ì„±ê³µ ì—¬ë¶€ (1=ì„±ê³µ, 0=ì‹¤íŒ¨)')
last_scrape_duration_seconds = Gauge('datahub_last_scrape_duration_seconds', 'ë§ˆì§€ë§‰ ìˆ˜ì§‘ ì†Œìš” ì‹œê°„ (ì´ˆ)')


def _auth_header():
    """ì¸ì¦ í—¤ë” ìƒì„±"""
    client_id = os.getenv('DATAHUB_CLIENT_ID')
    client_secret = os.getenv('DATAHUB_CLIENT_SECRET')
    
    headers = {"Content-Type": "application/json"}
    
    if client_id and client_secret:
        token = f"{client_id}:{client_secret}"
        b64_token = base64.b64encode(token.encode()).decode()
        headers["Authorization"] = f"Basic {b64_token}"
    
    return headers


def _post(query: str, variables: dict = None):
    """GraphQL ì¿¼ë¦¬ ì‹¤í–‰"""
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            payload = {"query": query}
            if variables:
                payload["variables"] = variables
            
            resp = requests.post(
                f"{GMS_URL}/api/graphql",
                json=payload,
                headers=_auth_header(),
                timeout=30
            )
            resp.raise_for_status()
            
            result = resp.json()
            
            if "errors" in result:
                log.error(f"GraphQL errors: {result['errors']}")
                scrape_errors.inc()
                return None
            
            return result.get("data")
        
        except Exception as e:
            log.error(f"Request error (attempt {attempt}/{MAX_RETRIES}): {e}")
            scrape_errors.inc()
            if attempt < MAX_RETRIES:
                time.sleep(RETRY_DELAY)
    
    return None


def get_entity_count(entity_type: str, filters: list = None):
    """ì—”í‹°í‹° ìˆ˜ ì¡°íšŒ (ë²”ìš©)"""
    filter_clause = ""
    if filters:
        filter_str = ", ".join([
            f'{{ field: "{f["field"]}", values: {f["values"]} }}'
            for f in filters
        ])
        filter_clause = f", filters: [{filter_str}]"
    
    query = f"""
    query {{
      search(
        input: {{
          type: {entity_type}
          query: "*"
          start: 0
          count: 1
          {filter_clause}
        }}
      ) {{
        total
      }}
    }}
    """
    
    payload = _post(query)
    
    if payload and 'search' in payload:
        return payload['search']['total']
    
    return 0


def get_datasets_by_platform():
    """í”Œë«í¼ë³„ ë°ì´í„°ì…‹ ìˆ˜"""
    # ì£¼ìš” í”Œë«í¼ ëª©ë¡
    platforms = [
        'postgres', 'mysql', 'oracle', 'mssql', 'mongodb',
        'snowflake', 'bigquery', 'redshift', 'databricks',
        'hive', 'spark', 'kafka', 's3', 'hdfs', 'glue'
    ]
    
    log.info("Fetching datasets by platform...")
    for platform in platforms:
        count = get_entity_count('DATASET', [
            {"field": "platform", "values": [f'"{platform}"']}
        ])
        if count > 0:
            datasets_by_platform.labels(platform=platform).set(count)
            log.info(f"  Platform {platform}: {count}")


def get_datasets_by_environment():
    """í™˜ê²½ë³„ ë°ì´í„°ì…‹ ìˆ˜ (PROD, DEV, QA ë“±)"""
    environments = ['PROD', 'DEV', 'QA', 'STAGING', 'UAT']
    
    log.info("Fetching datasets by environment...")
    for env in environments:
        count = get_entity_count('DATASET', [
            {"field": "origin", "values": [f'"{env}"']}
        ])
        if count > 0:
            datasets_by_env.labels(environment=env).set(count)
            log.info(f"  Environment {env}: {count}")


def get_top_tags():
    """ìƒìœ„ íƒœê·¸ë³„ ë°ì´í„°ì…‹ ìˆ˜"""
    # ëª¨ë“  íƒœê·¸ ì¡°íšŒ
    query = """
    query {
      search(
        input: {
          type: TAG
          query: "*"
          start: 0
          count: 50
        }
      ) {
        searchResults {
          entity {
            ... on Tag {
              name
              urn
            }
          }
        }
      }
    }
    """
    
    log.info("Fetching top tags...")
    payload = _post(query)
    
    if payload and 'search' in payload:
        tags = payload['search']['searchResults']
        
        for tag_result in tags[:20]:  # ìƒìœ„ 20ê°œë§Œ
            tag = tag_result['entity']
            tag_name = tag['name']
            tag_urn = tag['urn']
            
            # í•´ë‹¹ íƒœê·¸ë¥¼ ê°€ì§„ ë°ì´í„°ì…‹ ìˆ˜
            count = get_entity_count('DATASET', [
                {"field": "tags", "values": [f'"{tag_urn}"']}
            ])
            
            if count > 0:
                datasets_by_tag.labels(tag=tag_name).set(count)
                log.info(f"  Tag {tag_name}: {count} datasets")


def get_column_statistics():
    """ì»¬ëŸ¼ ë ˆë²¨ í†µê³„"""
    log.info("Fetching column-level statistics...")
    
    # ìŠ¤í‚¤ë§ˆê°€ ìˆëŠ” ë°ì´í„°ì…‹ë“¤ì„ ìƒ˜í”Œë§í•˜ì—¬ ì»¬ëŸ¼ í†µê³„ ê³„ì‚°
    query = """
    query {
      search(
        input: {
          type: DATASET
          query: "*"
          start: 0
          count: 100
        }
      ) {
        searchResults {
          entity {
            ... on Dataset {
              urn
              schemaMetadata {
                fields {
                  fieldPath
                  description
                }
              }
            }
          }
        }
      }
    }
    """
    
    payload = _post(query)
    
    if payload and 'search' in payload:
        results = payload['search']['searchResults']
        
        total_cols = 0
        cols_with_desc = 0
        
        for result in results:
            dataset = result['entity']
            schema = dataset.get('schemaMetadata')
            
            if schema and 'fields' in schema:
                fields = schema['fields']
                total_cols += len(fields)
                
                for field in fields:
                    if field.get('description'):
                        cols_with_desc += 1
        
        total_columns.set(total_cols)
        columns_with_description.set(cols_with_desc)
        columns_without_description.set(total_cols - cols_with_desc)
        
        log.info(f"  Total columns: {total_cols}")
        log.info(f"  Columns with description: {cols_with_desc}")


def get_recent_updates():
    """ìµœê·¼ ì—…ë°ì´íŠ¸ í†µê³„"""
    log.info("Fetching recent update statistics...")
    
    # ìµœê·¼ 1ì¼
    count_1d = get_entity_count('DATASET', [
        {"field": "lastModified", "values": ['"now-1d"']}
    ])
    datasets_updated_last_day.set(count_1d)
    log.info(f"  Updated in last 1 day: {count_1d}")
    
    # ìµœê·¼ 7ì¼
    count_7d = get_entity_count('DATASET', [
        {"field": "lastModified", "values": ['"now-7d"']}
    ])
    datasets_updated_last_week.set(count_7d)
    log.info(f"  Updated in last 7 days: {count_7d}")
    
    # ìµœê·¼ 30ì¼
    count_30d = get_entity_count('DATASET', [
        {"field": "lastModified", "values": ['"now-30d"']}
    ])
    datasets_updated_last_month.set(count_30d)
    log.info(f"  Updated in last 30 days: {count_30d}")


def collect_metrics():
    """ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
    start_time = time.time()
    
    log.info("=" * 70)
    log.info("Starting comprehensive metric collection...")
    log.info("=" * 70)
    
    try:
        # === 1. ê¸°ë³¸ ë°ì´í„°ì…‹ ë©”íŠ¸ë¦­ ===
        log.info("\n[1/12] Collecting basic dataset metrics...")
        total = get_entity_count('DATASET')
        total_datasets.set(total)
        log.info(f"âœ… Total datasets: {total}")
        
        # === 2. ë©”íƒ€ë°ì´í„° í’ˆì§ˆ ì§€í‘œ ===
        log.info("\n[2/12] Collecting metadata quality metrics...")
        
        without_owner = get_entity_count('DATASET', [
            {"field": "hasOwners", "values": ['"false"']}
        ])
        datasets_without_owner.set(without_owner)
        datasets_with_owner.set(max(0, total - without_owner))
        log.info(f"  Datasets without owner: {without_owner}")
        
        without_desc = get_entity_count('DATASET', [
            {"field": "hasDescription", "values": ['"false"']}
        ])
        datasets_without_description.set(without_desc)
        datasets_with_description.set(max(0, total - without_desc))
        log.info(f"  Datasets without description: {without_desc}")
        
        without_tags = get_entity_count('DATASET', [
            {"field": "hasTags", "values": ['"false"']}
        ])
        datasets_without_tags.set(without_tags)
        datasets_with_tags.set(max(0, total - without_tags))
        log.info(f"  Datasets without tags: {without_tags}")
        
        without_terms = get_entity_count('DATASET', [
            {"field": "hasGlossaryTerms", "values": ['"false"']}
        ])
        datasets_without_glossary_terms.set(without_terms)
        datasets_with_glossary_terms.set(max(0, total - without_terms))
        log.info(f"  Datasets without glossary terms: {without_terms}")
        
        # === 3. í”Œë«í¼ë³„ í†µê³„ ===
        log.info("\n[3/12] Collecting platform statistics...")
        get_datasets_by_platform()
        
        # === 4. í™˜ê²½ë³„ í†µê³„ ===
        log.info("\n[4/12] Collecting environment statistics...")
        get_datasets_by_environment()
        
        # === 5. íƒœê·¸ í†µê³„ ===
        log.info("\n[5/12] Collecting tag statistics...")
        tag_count = get_entity_count('TAG')
        total_tags.set(tag_count)
        log.info(f"  Total tags: {tag_count}")
        get_top_tags()
        
        # === 6. ìš©ì–´ì§‘ í†µê³„ ===
        log.info("\n[6/12] Collecting glossary statistics...")
        term_count = get_entity_count('GLOSSARY_TERM')
        total_glossary_terms.set(term_count)
        log.info(f"  Total glossary terms: {term_count}")
        
        # === 7. ë„ë©”ì¸ í†µê³„ ===
        log.info("\n[7/12] Collecting domain statistics...")
        domain_count = get_entity_count('DOMAIN')
        total_domains.set(domain_count)
        log.info(f"  Total domains: {domain_count}")
        
        # === 8. ì»¬ëŸ¼ ë ˆë²¨ í†µê³„ ===
        log.info("\n[8/12] Collecting column-level statistics...")
        get_column_statistics()
        
        # === 9. ìµœê·¼ ì—…ë°ì´íŠ¸ í†µê³„ ===
        log.info("\n[9/12] Collecting recent update statistics...")
        get_recent_updates()
        
        # === 10. ê¸°íƒ€ ì—”í‹°í‹° í†µê³„ ===
        log.info("\n[10/12] Collecting other entity statistics...")
        
        dashboard_count = get_entity_count('DASHBOARD')
        total_dashboards.set(dashboard_count)
        log.info(f"  Dashboards: {dashboard_count}")
        
        chart_count = get_entity_count('CHART')
        total_charts.set(chart_count)
        log.info(f"  Charts: {chart_count}")
        
        job_count = get_entity_count('DATA_JOB')
        total_data_jobs.set(job_count)
        log.info(f"  Data jobs: {job_count}")
        
        flow_count = get_entity_count('DATA_FLOW')
        total_data_flows.set(flow_count)
        log.info(f"  Data flows: {flow_count}")
        
        # === 11. ì‚¬ìš©ì ë° ê·¸ë£¹ ===
        log.info("\n[11/12] Collecting user and group statistics...")
        
        user_count = get_entity_count('CORP_USER')
        total_users.set(user_count)
        log.info(f"  Users: {user_count}")
        
        group_count = get_entity_count('CORP_GROUP')
        total_groups.set(group_count)
        log.info(f"  Groups: {group_count}")
        
        # === 12. ì™„ì„±ë„ ì§€í‘œ ê³„ì‚° ===
        log.info("\n[12/12] Calculating quality scores...")
        
        if total > 0:
            # ë©”íƒ€ë°ì´í„° ì™„ì„±ë„ (ì†Œìœ ì + ì„¤ëª… + íƒœê·¸ ëª¨ë‘ ìˆëŠ” ë¹„ìœ¨)
            max_incomplete = max(without_owner, without_desc, without_tags)
            completeness = (total - max_incomplete) / total
            metadata_completeness_ratio.set(completeness)
            log.info(f"  Metadata completeness: {completeness:.2%}")
            
            # ë¬¸ì„œí™” ì»¤ë²„ë¦¬ì§€ (ì„¤ëª… + ìš©ì–´ì§‘)
            max_undocumented = max(without_desc, without_terms)
            doc_coverage = (total - max_undocumented) / total
            documentation_coverage_ratio.set(doc_coverage)
            log.info(f"  Documentation coverage: {doc_coverage:.2%}")
            
            # ê±°ë²„ë„ŒìŠ¤ ì»¤ë²„ë¦¬ì§€ (ì†Œìœ ì + íƒœê·¸)
            max_ungoverned = max(without_owner, without_tags)
            gov_coverage = (total - max_ungoverned) / total
            governance_coverage_ratio.set(gov_coverage)
            log.info(f"  Governance coverage: {gov_coverage:.2%}")
        
        # ìˆ˜ì§‘ ì™„ë£Œ
        duration = time.time() - start_time
        last_scrape_duration_seconds.set(duration)
        last_scrape_success.set(1)
        
        log.info("=" * 70)
        log.info(f"âœ… Metric collection completed in {duration:.2f} seconds")
        log.info("=" * 70)
        
    except Exception as e:
        log.error(f"âŒ Error during metric collection: {e}", exc_info=True)
        last_scrape_success.set(0)
        scrape_errors.inc()


if __name__ == "__main__":
    log.info("=" * 70)
    log.info("DataHub Comprehensive Prometheus Exporter")
    log.info("=" * 70)
    log.info(f"GMS URL: {GMS_URL}")
    log.info(f"Scrape interval: {SCRAPE_INTERVAL} seconds")
    log.info(f"Metrics port: 9105")
    log.info("=" * 70)
    
    # Prometheus ì„œë²„ ì‹œì‘
    start_http_server(9105)
    log.info("âœ… Prometheus metrics server started")
    log.info(f"   Access metrics at: http://localhost:9105/metrics")
    
    # ì´ˆê¸° ë©”íŠ¸ë¦­ ì„¤ì •
    log.info("Setting initial metric values...")
    total_datasets.set(0)
    last_scrape_success.set(0)
    
    # ì²« ìˆ˜ì§‘ ì¦‰ì‹œ ì‹¤í–‰
    log.info("\nRunning initial collection...")
    try:
        collect_metrics()
    except Exception as e:
        log.error(f"Error in initial collection: {e}")
    
    # ì£¼ê¸°ì  ìˆ˜ì§‘
    while True:
        try:
            time.sleep(SCRAPE_INTERVAL)
            collect_metrics()
        except KeyboardInterrupt:
            log.info("\nğŸ‘‹ Shutting down gracefully...")
            break
        except Exception as e:
            log.error(f"Error in main loop: {e}", exc_info=True)
