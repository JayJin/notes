import os
import time
import logging
import base64
import requests
import json
from prometheus_client import start_http_server, Gauge, Counter

# ë¡œê¹… ì„¤ì •
log = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
DATAHUB_GMS_HOST = os.getenv('DATAHUB_GMS_HOST', 'datahub-gms')
DATAHUB_GMS_PORT = os.getenv('DATAHUB_GMS_PORT', '8080')
GMS_URL = f"http://{DATAHUB_GMS_HOST}:{DATAHUB_GMS_PORT}"

PAGE_SIZE = 500
MAX_RETRIES = 3
RETRY_DELAY = 5
SCRAPE_INTERVAL = int(os.getenv('SCRAPE_INTERVAL', '60'))

# Prometheus Metrics
scrape_errors = Counter('datahub_scrape_errors_total', 'Total scrape errors')

total_datasets = Gauge('datahub_datasets_total', 'ì „ì²´ ë°ì´í„°ì…‹ ìˆ˜')
datasets_by_platform = Gauge('datahub_datasets_by_platform', 'í”Œë«í¼ë³„ ë°ì´í„°ì…‹ ìˆ˜', ['platform'])

datasets_with_owner = Gauge('datahub_datasets_with_owner', 'ì†Œìœ ìê°€ ìˆëŠ” ë°ì´í„°ì…‹')
datasets_without_owner = Gauge('datahub_datasets_without_owner', 'ì†Œìœ ìê°€ ì—†ëŠ” ë°ì´í„°ì…‹')
datasets_with_description = Gauge('datahub_datasets_with_description', 'ì„¤ëª…ì´ ìˆëŠ” ë°ì´í„°ì…‹')
datasets_without_description = Gauge('datahub_datasets_without_description', 'ì„¤ëª…ì´ ì—†ëŠ” ë°ì´í„°ì…‹')
datasets_with_tags = Gauge('datahub_datasets_with_tags', 'íƒœê·¸ê°€ ìˆëŠ” ë°ì´í„°ì…‹')
datasets_without_tags = Gauge('datahub_datasets_without_tags', 'íƒœê·¸ê°€ ì—†ëŠ” ë°ì´í„°ì…‹')

total_tags = Gauge('datahub_tags_total', 'ì „ì²´ íƒœê·¸ ìˆ˜')
total_glossary_terms = Gauge('datahub_glossary_terms_total', 'ì „ì²´ ìš©ì–´ì§‘ ìˆ˜')
total_domains = Gauge('datahub_domains_total', 'ì „ì²´ ë„ë©”ì¸ ìˆ˜')

datasets_updated_last_week = Gauge('datahub_datasets_updated_last_week', 'ìµœê·¼ 7ì¼ ë‚´ ì—…ë°ì´íŠ¸')

total_dashboards = Gauge('datahub_dashboards_total', 'ì „ì²´ ëŒ€ì‹œë³´ë“œ ìˆ˜')
total_charts = Gauge('datahub_charts_total', 'ì „ì²´ ì°¨íŠ¸ ìˆ˜')
total_data_jobs = Gauge('datahub_data_jobs_total', 'ì „ì²´ ë°ì´í„° ì‘ì—… ìˆ˜')
total_users = Gauge('datahub_users_total', 'ì „ì²´ ì‚¬ìš©ì ìˆ˜')
total_groups = Gauge('datahub_groups_total', 'ì „ì²´ ê·¸ë£¹ ìˆ˜')

metadata_completeness_ratio = Gauge('datahub_metadata_completeness_ratio', 'ë©”íƒ€ë°ì´í„° ì™„ì„±ë„')
documentation_coverage_ratio = Gauge('datahub_documentation_coverage_ratio', 'ë¬¸ì„œí™” ì»¤ë²„ë¦¬ì§€')

last_scrape_success = Gauge('datahub_last_scrape_success', 'ë§ˆì§€ë§‰ ìˆ˜ì§‘ ì„±ê³µ ì—¬ë¶€')
last_scrape_duration_seconds = Gauge('datahub_last_scrape_duration_seconds', 'ë§ˆì§€ë§‰ ìˆ˜ì§‘ ì†Œìš” ì‹œê°„')


def _auth_header():
    """ì¸ì¦ í—¤ë” ìƒì„±"""
    client_id = os.getenv('DATAHUB_CLIENT_ID')
    client_secret = os.getenv('DATAHUB_CLIENT_SECRET')
    
    headers = {"Content-Type": "application/json"}
    
    if client_id and client_secret:
        token = f"{client_id}:{client_secret}"
        b64_token = base64.b64encode(token.encode()).decode()
        headers["Authorization"] = f"Basic {b64_token}"
    
    return headers


def _post(query: str, variables: dict = None):
    """GraphQL ì¿¼ë¦¬ ì‹¤í–‰"""
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            payload = {"query": query}
            if variables:
                payload["variables"] = variables
            
            log.debug(f"Sending query: {query[:200]}...")
            
            resp = requests.post(
                f"{GMS_URL}/api/graphql",
                json=payload,
                headers=_auth_header(),
                timeout=30
            )
            
            # 500 ì—ëŸ¬ ì‹œ ì‘ë‹µ ë‚´ìš© ì¶œë ¥
            if resp.status_code == 500:
                log.error(f"500 Server Error - Response: {resp.text[:500]}")
                scrape_errors.inc()
                return None
            
            resp.raise_for_status()
            
            result = resp.json()
            
            if "errors" in result:
                log.error(f"GraphQL errors: {result['errors']}")
                scrape_errors.inc()
                return None
            
            return result.get("data")
        
        except requests.exceptions.HTTPError as e:
            log.error(f"HTTP error (attempt {attempt}/{MAX_RETRIES}): {e}")
            log.error(f"Response: {resp.text[:500] if 'resp' in locals() else 'No response'}")
            scrape_errors.inc()
            if attempt < MAX_RETRIES:
                time.sleep(RETRY_DELAY)
        
        except Exception as e:
            log.error(f"Request error (attempt {attempt}/{MAX_RETRIES}): {e}")
            scrape_errors.inc()
            if attempt < MAX_RETRIES:
                time.sleep(RETRY_DELAY)
    
    return None


def get_entity_count_simple(entity_type: str):
    """âœ… ë‹¨ìˆœ ì—”í‹°í‹° ìˆ˜ ì¡°íšŒ (í•„í„° ì—†ìŒ)"""
    query = f"""
    query {{
      search(
        input: {{
          type: {entity_type}
          query: "*"
          start: 0
          count: 1
        }}
      ) {{
        total
      }}
    }}
    """
    
    payload = _post(query)
    
    if payload and 'search' in payload:
        return payload['search']['total']
    
    return 0


def get_datasets_without_field(field_name: str):
    """âœ… íŠ¹ì • í•„ë“œê°€ ì—†ëŠ” ë°ì´í„°ì…‹ ìˆ˜"""
    # DataHub GraphQLì—ì„œ ì§€ì›í•˜ëŠ” í•„í„° í•„ë“œ
    valid_filters = {
        'owner': 'hasOwners',
        'description': 'hasDescription', 
        'tags': 'hasTags',
        'glossary': 'hasGlossaryTerms'
    }
    
    filter_field = valid_filters.get(field_name)
    
    if not filter_field:
        log.warning(f"Unknown field: {field_name}")
        return 0
    
    query = f"""
    query {{
      search(
        input: {{
          type: DATASET
          query: "*"
          filters: [
            {{
              field: "{filter_field}"
              values: ["false"]
            }}
          ]
          start: 0
          count: 1
        }}
      ) {{
        total
      }}
    }}
    """
    
    payload = _post(query)
    
    if payload and 'search' in payload:
        return payload['search']['total']
    
    return 0


def get_datasets_by_platform():
    """âœ… í”Œë«í¼ë³„ ë°ì´í„°ì…‹ ìˆ˜ (ê°„ë‹¨í•œ ë°©ì‹)"""
    platforms = ['postgres', 'mysql', 'snowflake', 'bigquery', 'kafka']
    
    log.info("Fetching datasets by platform...")
    
    for platform in platforms:
        query = f"""
        query {{
          search(
            input: {{
              type: DATASET
              query: "platform:{platform}"
              start: 0
              count: 1
            }}
          ) {{
            total
          }}
        }}
        """
        
        payload = _post(query)
        
        if payload and 'search' in payload:
            count = payload['search']['total']
            if count > 0:
                datasets_by_platform.labels(platform=platform).set(count)
                log.info(f"  Platform {platform}: {count}")


def collect_metrics():
    """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (ì•ˆì •í™”ëœ ë²„ì „)"""
    start_time = time.time()
    
    log.info("=" * 70)
    log.info("Starting metric collection...")
    log.info("=" * 70)
    
    try:
        # === 1. ì „ì²´ ë°ì´í„°ì…‹ ìˆ˜ ===
        log.info("\n[1/10] Total datasets...")
        total = get_entity_count_simple('DATASET')
        total_datasets.set(total)
        log.info(f"âœ… Total datasets: {total}")
        
        # === 2. ë©”íƒ€ë°ì´í„° í’ˆì§ˆ ì§€í‘œ ===
        log.info("\n[2/10] Metadata quality metrics...")
        
        without_owner = get_datasets_without_field('owner')
        datasets_without_owner.set(without_owner)
        datasets_with_owner.set(max(0, total - without_owner))
        log.info(f"  Without owner: {without_owner}")
        
        without_desc = get_datasets_without_field('description')
        datasets_without_description.set(without_desc)
        datasets_with_description.set(max(0, total - without_desc))
        log.info(f"  Without description: {without_desc}")
        
        without_tags = get_datasets_without_field('tags')
        datasets_without_tags.set(without_tags)
        datasets_with_tags.set(max(0, total - without_tags))
        log.info(f"  Without tags: {without_tags}")
        
        # === 3. í”Œë«í¼ë³„ í†µê³„ ===
        log.info("\n[3/10] Platform statistics...")
        get_datasets_by_platform()
        
        # === 4. íƒœê·¸ ìˆ˜ ===
        log.info("\n[4/10] Tag count...")
        tag_count = get_entity_count_simple('TAG')
        total_tags.set(tag_count)
        log.info(f"  Total tags: {tag_count}")
        
        # === 5. ìš©ì–´ì§‘ ìˆ˜ ===
        log.info("\n[5/10] Glossary term count...")
        term_count = get_entity_count_simple('GLOSSARY_TERM')
        total_glossary_terms.set(term_count)
        log.info(f"  Total glossary terms: {term_count}")
        
        # === 6. ë„ë©”ì¸ ìˆ˜ ===
        log.info("\n[6/10] Domain count...")
        domain_count = get_entity_count_simple('DOMAIN')
        total_domains.set(domain_count)
        log.info(f"  Total domains: {domain_count}")
        
        # === 7. ëŒ€ì‹œë³´ë“œ & ì°¨íŠ¸ ===
        log.info("\n[7/10] Dashboard and chart counts...")
        dashboard_count = get_entity_count_simple('DASHBOARD')
        total_dashboards.set(dashboard_count)
        log.info(f"  Dashboards: {dashboard_count}")
        
        chart_count = get_entity_count_simple('CHART')
        total_charts.set(chart_count)
        log.info(f"  Charts: {chart_count}")
        
        # === 8. ë°ì´í„° ì‘ì—… ===
        log.info("\n[8/10] Data job count...")
        job_count = get_entity_count_simple('DATA_JOB')
        total_data_jobs.set(job_count)
        log.info(f"  Data jobs: {job_count}")
        
        # === 9. ì‚¬ìš©ì & ê·¸ë£¹ ===
        log.info("\n[9/10] User and group counts...")
        user_count = get_entity_count_simple('CORP_USER')
        total_users.set(user_count)
        log.info(f"  Users: {user_count}")
        
        group_count = get_entity_count_simple('CORP_GROUP')
        total_groups.set(group_count)
        log.info(f"  Groups: {group_count}")
        
        # === 10. ì™„ì„±ë„ ì§€í‘œ ===
        log.info("\n[10/10] Calculating quality scores...")
        
        if total > 0:
            # ë©”íƒ€ë°ì´í„° ì™„ì„±ë„
            max_incomplete = max(without_owner, without_desc, without_tags)
            completeness = (total - max_incomplete) / total
            metadata_completeness_ratio.set(completeness)
            log.info(f"  Metadata completeness: {completeness:.2%}")
            
            # ë¬¸ì„œí™” ì»¤ë²„ë¦¬ì§€
            doc_coverage = (total - without_desc) / total
            documentation_coverage_ratio.set(doc_coverage)
            log.info(f"  Documentation coverage: {doc_coverage:.2%}")
        
        # ìˆ˜ì§‘ ì™„ë£Œ
        duration = time.time() - start_time
        last_scrape_duration_seconds.set(duration)
        last_scrape_success.set(1)
        
        log.info("=" * 70)
        log.info(f"âœ… Collection completed in {duration:.2f}s")
        log.info("=" * 70)
        
    except Exception as e:
        log.error(f"âŒ Error: {e}", exc_info=True)
        last_scrape_success.set(0)
        scrape_errors.inc()


if __name__ == "__main__":
    log.info("=" * 70)
    log.info("DataHub Prometheus Exporter (Stable Version)")
    log.info("=" * 70)
    log.info(f"GMS URL: {GMS_URL}")
    log.info(f"Scrape interval: {SCRAPE_INTERVAL}s")
    log.info(f"Metrics port: 9105")
    log.info("=" * 70)
    
    # Prometheus ì„œë²„ ì‹œì‘
    start_http_server(9105)
    log.info("âœ… Metrics server started")
    log.info(f"   http://localhost:9105/metrics")
    
    # ì´ˆê¸°ê°’ ì„¤ì •
    total_datasets.set(0)
    last_scrape_success.set(0)
    
    # ì²« ìˆ˜ì§‘
    log.info("\nRunning initial collection...")
    try:
        collect_metrics()
    except Exception as e:
        log.error(f"Initial collection error: {e}")
    
    # ì£¼ê¸°ì  ìˆ˜ì§‘
    while True:
        try:
            time.sleep(SCRAPE_INTERVAL)
            collect_metrics()
        except KeyboardInterrupt:
            log.info("\nğŸ‘‹ Shutting down...")
            break
        except Exception as e:
            log.error(f"Main loop error: {e}", exc_info=True)
