docker exec datahub-airflow-scheduler-1 curl -X POST \
  'http://datahub-gms:8080/aspects?action=ingestProposal' \
  -H 'Content-Type: application/json' \
  -d '{
    "proposal": {
      "entityType": "dataset",
      "entityUrn": "urn:li:dataset:(urn:li:dataPlatform:postgres,cdc.public.pd,PROD)",
      "aspectName": "editableDatasetProperties",
      "changeType": "UPSERT",
      "aspect": {
        "value": "{\"description\":\"REST API 테스트 설명\"}",
        "contentType": "application/json"
      }
    }
  }'

또는

docker exec datahub-airflow-scheduler-1 curl -X POST http://datahub-gms:8080/api/graphql \
  -H "Content-Type: application/json" \
  -d '{
    "query": "mutation { updateDataset(urn: \"urn:li:dataset:(urn:li:dataPlatform:postgres,cdc.public.pd,PROD)\", input: { editableProperties: { description: \"GraphQL 직접 테스트\" } }) { urn } }"
  }' | python3 -m json.tool

  그리고 다시 확인

  docker exec datahub-airflow-scheduler-1 curl -s -X POST http://datahub-gms:8080/api/graphql \
  -H "Content-Type: application/json" \
  -d '{
    "query": "{ dataset(urn: \"urn:li:dataset:(urn:li:dataPlatform:postgres,cdc.public.pd,PROD)\") { editableProperties { description } } }"
  }' | python3 -m json.tool




========================================================================================
3. DAG 코드 수정 (REST API 사용)
GraphQL이 제대로 작동하지 않는다면 REST API로 변경:

python
def update_dataset_description(urn, description):
    """REST API로 테이블 설명 업데이트"""
    import json
    
    payload = {
        "proposal": {
            "entityType": "dataset",
            "entityUrn": urn,
            "aspectName": "editableDatasetProperties",
            "changeType": "UPSERT",
            "aspect": {
                "value": json.dumps({"description": description}),
                "contentType": "application/json"
            }
        }
    }
    
    try:
        resp = requests.post(
            f"{DATAHUB_GMS_URL}/aspects?action=ingestProposal",
            json=payload,
            headers={'Content-Type': 'application/json'},
            timeout=30
        )
        
        if resp.status_code == 200:
            logger.info(f"✓ Updated dataset: {urn[:60]}")
            return True
        else:
            logger.error(f"Failed: {resp.status_code} - {resp.text[:200]}")
            return False
    except Exception as e:
        logger.error(f"Error: {e}")
        return False


def update_field_description(dataset_urn, field_path, description):
    """REST API로 컬럼 설명 업데이트"""
    import json
    
    # 먼저 기존 editableSchemaMetadata 조회
    existing_fields = []
    
    # 새 필드 정보 추가
    field_info = {
        "fieldPath": field_path,
        "description": description
    }
    
    payload = {
        "proposal": {
            "entityType": "dataset",
            "entityUrn": dataset_urn,
            "aspectName": "editableSchemaMetadata",
            "changeType": "UPSERT",
            "aspect": {
                "value": json.dumps({
                    "editableSchemaFieldInfo": [field_info]
                }),
                "contentType": "application/json"
            }
        }
    }
    
    try:
        resp = requests.post(
            f"{DATAHUB_GMS_URL}/aspects?action=ingestProposal",
            json=payload,
            headers={'Content-Type': 'application/json'},
            timeout=30
        )
        
        if resp.status_code == 200:
            logger.info(f"✓ Updated field: {field_path}")
            return True
        else:
            logger.error(f"Failed: {resp.status_code} - {resp.text[:200]}")
            return False
    except Exception as e:
        logger.error(f"Error: {e}")
        return False
4. 완전히 새로운 DAG 파일 (REST API 기반)
bash
docker exec -i datahub-airflow-scheduler-1 bash -c 'cat > /opt/airflow/dags/datahub_dag.py' << 'EOF'
import os
import csv
import json
import requests
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
import logging

logger = logging.getLogger(__name__)

DATAHUB_GMS_URL = os.getenv('DATAHUB_GMS_URL', 'http://datahub-gms:8080')
CSV_FILE_PATH = '/tmp/datahub_metadata_update.csv'

default_args = {
    'owner': 'data-team',
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'datahub_metadata_bulk_update',
    default_args=default_args,
    description='DataHub 메타데이터 업데이트 (REST API)',
    schedule_interval=None,
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['datahub'],
)

def update_dataset_description(urn, description):
    """테이블 설명 업데이트"""
    payload = {
        "proposal": {
            "entityType": "dataset",
            "entityUrn": urn,
            "aspectName": "editableDatasetProperties",
            "changeType": "UPSERT",
            "aspect": {
                "value": json.dumps({"description": description}),
                "contentType": "application/json"
            }
        }
    }
    
    try:
        resp = requests.post(
            f"{DATAHUB_GMS_URL}/aspects?action=ingestProposal",
            json=payload,
            timeout=30
        )
        
        if resp.status_code == 200:
            logger.info(f"✓ Updated dataset: {urn[:60]}")
            return True
        logger.error(f"Failed {resp.status_code}: {resp.text[:200]}")
        return False
    except Exception as e:
        logger.error(f"Error: {e}")
        return False

def update_field_description(dataset_urn, field_path, description):
    """컬럼 설명 업데이트"""
    payload = {
        "proposal": {
            "entityType": "dataset",
            "entityUrn": dataset_urn,
            "aspectName": "editableSchemaMetadata",
            "changeType": "UPSERT",
            "aspect": {
                "value": json.dumps({
                    "editableSchemaFieldInfo": [{
                        "fieldPath": field_path,
                        "description": description
                    }]
                }),
                "contentType": "application/json"
            }
        }
    }
    
    try:
        resp = requests.post(
            f"{DATAHUB_GMS_URL}/aspects?action=ingestProposal",
            json=payload,
            timeout=30
        )
        
        if resp.status_code == 200:
            logger.info(f"✓ Updated field: {field_path}")
            return True
        logger.error(f"Failed {resp.status_code}: {resp.text[:200]}")
        return False
    except Exception as e:
        logger.error(f"Error: {e}")
        return False

def validate_connection():
    """연결 테스트"""
    logger.info(f"Testing: {DATAHUB_GMS_URL}")
    try:
        resp = requests.get(f"{DATAHUB_GMS_URL}/health", timeout=10)
        if resp.status_code == 200:
            logger.info("✓ Connected")
            return True
    except Exception as e:
        logger.error(f"Failed: {e}")
        return False

def process_csv():
    """CSV 처리"""
    if not os.path.exists(CSV_FILE_PATH):
        logger.error(f"CSV not found: {CSV_FILE_PATH}")
        return
    
    logger.info(f"Reading: {CSV_FILE_PATH}")
    success = fail = 0
    
    with open(CSV_FILE_PATH, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for idx, row in enumerate(reader, 1):
            row_type = row.get('type', '').strip().lower()
            urn = row.get('urn', '').strip()
            field_path = row.get('field_path', '').strip()
            description = row.get('description', '').strip()
            
            if not urn:
                continue
            
            logger.info(f"Row {idx}: {row_type}")
            
            if row_type == 'dataset' and description:
                if update_dataset_description(urn, description):
                    success += 1
                else:
                    fail += 1
            
            elif row_type == 'column' and field_path and description:
                if update_field_description(urn, field_path, description):
                    success += 1
                else:
                    fail += 1
    
    logger.info(f"Done! Success: {success}, Failed: {fail}")

task_validate = PythonOperator(
    task_id='validate',
    python_callable=validate_connection,
    dag=dag,
)

task_process = PythonOperator(
    task_id='process_csv',
    python_callable=process_csv,
    dag=dag,
)

task_validate >> task_process
EOF
5. Scheduler 재시작 및 재실행
bash
# Scheduler 재시작
docker restart datahub-airflow-scheduler-1

# 30초 대기
sleep 30

# DAG 트리거
docker exec datahub-airflow-scheduler-1 airflow dags trigger datahub_metadata_bulk_update
6. 다시 확인
bash
docker exec datahub-airflow-scheduler-1 curl -s -X POST http://datahub-gms:8080/api/graphql \
  -H "Content-Type: application/json" \
  -d '{
    "query": "{ dataset(urn: \"urn:li:dataset:(urn:li:dataPlatform:postgres,cdc.public.pd,PROD)\") { editableProperties { description } } }"
  }' | python3 -m json.tool